# --------------------------------------------------------
# Swing-Up `Dyna-NN` benchmark as part of ArXiv Section 5.1
# for use with `pbt_dyna.py`
# --------------------------------------------------------

exp_dir: cart-swingup
n_train_iter: 40000
dyn_fit_freq: 40
fcnet_hiddens: [64, 64]
# use_pbt: False

# used for rolling out new on-policy data for 
# dynamics fitting
real_env:
  class: DMCEnvWrapper
  config: 
      domain_name: "cartpole"
      task_name: "swingup"
      frame_skip: 1
      from_pixels: False
      task_kwargs:
        time_limit: 10 # DEFAULT IS 10!
drl:
  class: PPO
  config:
    training: 
      lr_schedule: [[0, 3.0e-4], [10000000, 3.0e-9]]
      gamma: 0.99
      lambda_: 0.95
      vf_loss_coeff: 0.5
      vf_clip_param: 0.2
      clip_param: 0.2
      grad_clip: 0.5
    environment:
      env: 
      env_config:
        max_episode_steps: 1000
        real_env_class: DMCEnvWrapper
        real_env_config: 
          domain_name: "cartpole"
          task_name: "swingup"
          frame_skip: 1
          from_pixels: False
          task_kwargs:
            time_limit: 10 # DEFAULT IS 10!
        init_real_on_start: True
        use_real_env: False
        ensemble_modes: 
          dyn: mean
          rew: # None
        init_weights: True
        act_dim: 1
        obs_dim: 5
        act_bounds: 
          - [-1, 1]
        obs_bounds: 
          - [-5, 5]     # pos
          - [-1.1, 1.1] # cos(th)
          - [-1.1, 1.1] # sin(th)
          - [-10, 10]   # dx
          - [-10, 10]   # dth
    framework: torch
    evaluation: 
      evaluation_config:
        env_config:
          max_episode_steps: 1000
          init_real_on_start: True
          use_real_env: True
          use_bounds: False
          real_env_class: DMCEnvWrapper
          real_env_config: 
            domain_name: "cartpole"
            task_name: "swingup"
            from_pixels: False
            task_kwargs:
              time_limit: 10 # DEFAULT IS 10!
        explore: False 
      evaluation_interval: 1
      evaluation_duration: 5
      evaluation_duration_unit: "episodes"
      always_attach_evaluation_results: True


off_policy_buffer:
  config:
    max_traj:
    max_samples:
  init: 
    type: collect
    kwargs: 
      n_steps: 8000
      n_steps_reset: 1000

on_policy_buffer:
  config:
    max_traj: 
    max_samples:  # None
  collect:
    n_steps: 1000
    n_steps_reset: 2000

dynamics_model:
  class: EnsembleNetDynamicsModel
  config:
    n_models: 5
    single_model_config: 
      callbacks: 'project_cartpole'
      n_epochs: 50
      nn_kwargs:
        n_input: 6 # = 5 state + 1 ctrl
        n_output: 5
        hidden_size: 64


rew_model:
  class: FunctionalRewardModel
  config: 
    name: cart_reward

ray_config:
    run_config:
        name: "dyna-nn_freq=40_len=1000_coll=15k-inf_ens=mean-5_epoch=50_steps=1000"
        stop:
            num_env_steps_sampled: 1.0e+7
        log_to_file: True 
    tune_config:
        num_samples: 20 # number of sample trials to run
    checkpoint_freq: 10
